{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Likelihood-Free Markov Chain Monte Carlo\n",
    "\n",
    "#### Brett Morris\n",
    "\n",
    "### In this tutorial \n",
    "\n",
    "We will write our own Markov Chain Monte Carlo algorithm with a Metropolis-Hastings sampler, then write a likelihood-free version.\n",
    "\n",
    "\n",
    "## 1) MCMC with likelihoods\n",
    "\n",
    "Sometimes it's feasible to write down a [likelihood](https://en.wikipedia.org/wiki/Likelihood_function). In this example, we can use the $\\chi^2$ to measure the goodness-of-fit, and use the [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm) algorithm to sample from the posterior distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the properties of the line that we'll be fitting: \n",
    "true_std = 3\n",
    "true_intercept = 0.5\n",
    "\n",
    "x = np.linspace(0, 10, 500)\n",
    "x -= x.mean()\n",
    "y = true_std * np.random.randn(len(x)) + true_intercept\n",
    "yerr = true_std * np.ones_like(x)\n",
    "\n",
    "plt.errorbar(x, y, yerr, fmt='.')\n",
    "plt.plot(x, true_intercept + np.zeros_like(x), color='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define a simple linear model to describe the data, which has parameters $\\theta = \\{m, b\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def linear_model(theta, x): \n",
    "    \"\"\"\n",
    "    Simple linear model. \n",
    "    \"\"\"\n",
    "    m, b = theta\n",
    "    return m * x + b\n",
    "\n",
    "\n",
    "def chi2(theta, x, y, yerr, model): \n",
    "    \"\"\"\n",
    "    Compute the \\chi^2 by comparing `model` evaluated at `theta`\n",
    "    to `y` at each value of `x`. \n",
    "    \"\"\"\n",
    "    return np.sum((model(theta, x) - y)**2 / yerr**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let the proposal step simply draw from a Gaussian distribution:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def proposal(theta, scale=1):\n",
    "    \"\"\"\n",
    "    Generate proposal step, by adding a draw from a \n",
    "    Gaussian distribution to the initial step position\n",
    "    \"\"\"\n",
    "    return theta + scale * np.random.randn(len(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will decide whether or not to accept new proposal steps using the Metropolis-Hastings algorithm, as defined by [Ford (2005)](https://arxiv.org/abs/astro-ph/0305441):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def metropolis_hastings(init_theta, x, y, yerr, acceptance, scale=0.05):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings algorithm, a la Ford (2005). \n",
    "    \n",
    "    1) Generate a proposal step\n",
    "    2) Compare the chi^2 of the proposal step to the current step\n",
    "    3) Draw a random number `u` on [0, 1]\n",
    "    4) Compute alpha = min([exp(-0.5 * (chi2(new) - chi2(old)), 1])\n",
    "    5) If u <= alpha, accept step, otherwise keep step\n",
    "    \"\"\"\n",
    "    # Generate a proposal step: \n",
    "    proposed_theta = proposal(init_theta, scale=scale)\n",
    "\n",
    "    # Compare chi^2 of proposed step to current step:\n",
    "    chi2_init_step = chi2(init_theta, x, y, yerr, linear_model)\n",
    "    chi2_proposed_step = chi2(proposed_theta, x, y, yerr, linear_model)\n",
    "    relative_likelihood = np.exp(-0.5 * (chi2_proposed_step - chi2_init_step))\n",
    "    \n",
    "    alpha = np.min([relative_likelihood, 1])\n",
    "\n",
    "    # If U(0, 1) <= alpha, accept the step: \n",
    "    if np.random.rand() <= alpha: \n",
    "        return proposed_theta, acceptance + 1\n",
    "    else: \n",
    "        return init_theta, acceptance\n",
    "    \n",
    "\n",
    "def sampler(x, y, yerr, init_theta, n_steps, scale=0.05):\n",
    "    \"\"\"\n",
    "    Markov Chain Monte Carlo sampler. \n",
    "    \"\"\"\n",
    "    current_theta = np.copy(init_theta)\n",
    "    # Allocate memory for samples: \n",
    "    samples = np.zeros((n_steps, len(init_theta)))\n",
    "    samples[0, :] = init_theta\n",
    "    acceptance = 0\n",
    "    \n",
    "    for i in range(1, n_steps):\n",
    "        # Run the M-H algorithm to determine next step:\n",
    "        current_theta, acceptance = metropolis_hastings(current_theta, \n",
    "                                                        x, y, yerr, acceptance, \n",
    "                                                        scale=scale)\n",
    "        # Record the result: \n",
    "        samples[i, :] = current_theta\n",
    "        \n",
    "    # Compute the final acceptance rate\n",
    "    acceptance_rate = acceptance / n_steps\n",
    "    \n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make an initial guess and let the MCMC algorithm find the correct solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "init_parameters = [0, 0.5]  # slope, intercept\n",
    "n_steps = 50000\n",
    "\n",
    "# This tweakable parameter determines how\n",
    "# far new steps should be taken away from \n",
    "# previous steps. Increase `scale` to \n",
    "# decrease your acceptance rate: \n",
    "scale = 0.08\n",
    "\n",
    "samples, acceptance_rate = sampler(x, y, yerr, init_parameters, \n",
    "                                   n_steps, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What is the acceptance rate? Ideally this should be near 45%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(acceptance_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's plot a few random draws from the posterior probability distribution functions for each parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100): \n",
    "    random_step = np.random.randint(0, samples.shape[0])\n",
    "    random_theta = samples[random_step, :]\n",
    "    plt.plot(x, linear_model(random_theta, x), alpha=0.05, color='k')\n",
    "plt.errorbar(x, y, yerr, fmt='.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that the uncertainty in the measurements is being reflected by uncertainty in the slope and intercept parameters. \n",
    "\n",
    "We normally see the posterior probability distribution functions displayed in a \"corner\" plot like the one below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "\n",
    "burned_in_samples = samples[1000:]\n",
    "\n",
    "corner(burned_in_samples, labels=['m', 'b'], truths=[0, true_intercept]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that the posterior distributions are consistent within the uncertainties for each parameter! We are accurately measuring each parameter and their uncertainties. \n",
    "\n",
    "In this example the likelihood was easy to compute, but there are cases when it's expensive or impossible to compute the likelihood. When that's the case, you can use...\n",
    "\n",
    "## 2) *Likelihood-Free* MCMC\n",
    "\n",
    "Following the algorithm outlined by [Marjoram et al. 2003](http://www.pnas.org/content/100/26/15324).\n",
    "\n",
    "Likelihood-free MCMC bypasses the step within the M-H algorithm where you compare the likelihood of the observations given the parameters. Instead, you: \n",
    "\n",
    "1. propose new parameters $\\theta^\\prime$\n",
    "\n",
    "2. generate a simulated dataset $y^\\prime \\equiv D^\\prime$ from the new parameters\n",
    "\n",
    "3. measure the \"distance\" $\\rho(D, D^\\prime)$ between the simulated and observed datasets\n",
    "\n",
    "4. if the distance is less than some tolerance $h$, accept the step, return to (1)\n",
    "\n",
    "In this example, we'll be using a model that generates simulated datasets $D^\\prime$ from a normal distribution, and computes the Euclidian *distance* between the means and standard deviations of the observations $D$ and the simulated dataset $D^\\prime$, \n",
    "\n",
    "$$ \\rho(D, D^\\prime) = \\sqrt{\\left(\\bar{y_\\mathrm{obs}} - \\bar{y^\\prime}\\right)^2 + \\left(\\mathrm{std}(y_\\mathrm{obs}) - \\mathrm{std}(y^\\prime)\\right)^2},$$\n",
    "\n",
    "and accept the step if $\\rho(D, D^\\prime) \\leq h_j$ for $h \\in \\{1, 0.5, 0.2\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_prime(theta): \n",
    "    \"\"\"\n",
    "    Simulate a dataset by drawing from a random normal distribution\n",
    "    \"\"\"\n",
    "    mean, std = theta\n",
    "    return mean + std * np.random.randn(len(x))\n",
    "\n",
    "def rho(theta, x, y, yerr):\n",
    "    \"\"\"\n",
    "    Distance metric\n",
    "    \"\"\"\n",
    "    # Generate a simulated dataset for comparison with observations\n",
    "    simulated = D_prime(theta)\n",
    "    \n",
    "    # Compute distance between observations and simulation\n",
    "    distance = np.sqrt((simulated.mean() - y.mean())**2 + (simulated.std() - y.std())**2)\n",
    "    return distance\n",
    "\n",
    "    \n",
    "def metropolis_hastings_marjoram(init_theta, x, y, yerr, acceptance, scale, h):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings algorithm, a la Marjoram et al. (2003)\n",
    "    \"\"\"\n",
    "    # Generate a proposal step: \n",
    "    proposed_theta = proposal(init_theta, scale=scale)\n",
    "\n",
    "    # Compute distance of proposed step\n",
    "    rho_proposed_step = rho(proposed_theta, x, y, yerr)\n",
    "    \n",
    "    # If distance is less than h, accept step\n",
    "    if rho_proposed_step <= h: \n",
    "        return proposed_theta, acceptance + 1\n",
    "\n",
    "    else: \n",
    "        return init_theta, acceptance\n",
    "\n",
    "def sampler_marjoram(x, y, yerr, init_theta, n_steps, scale, h):\n",
    "    \"\"\"\n",
    "    Markov Chain Monte Carlo sampler without likelihoods \n",
    "    \"\"\"\n",
    "    current_theta = np.copy(init_theta)\n",
    "    # Allocate memory for samples: \n",
    "    samples = np.zeros((n_steps, len(init_theta)))\n",
    "    samples[0, :] = init_theta\n",
    "    acceptance = 0\n",
    "    \n",
    "    for i in range(1, n_steps):\n",
    "        # Run the M-H algorithm to determine next step:\n",
    "        current_theta, acceptance = metropolis_hastings_marjoram(current_theta, \n",
    "                                                                 x, y, yerr, acceptance, \n",
    "                                                                 scale, h)\n",
    "        # Record the result: \n",
    "        samples[i, :] = current_theta\n",
    "        \n",
    "    # Compute the final acceptance rate\n",
    "    acceptance_rate = acceptance / n_steps\n",
    "    \n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop over several values for $h$ and see how the posterior changes as $h \\rightarrow 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parameters = [y.mean(), y.std()]  # mean, std\n",
    "n_steps = 100000\n",
    "\n",
    "# This tweakable parameter determines how\n",
    "# far new steps should be taken away from \n",
    "# previous steps. Increase `scale` to \n",
    "# decrease your acceptance rate: \n",
    "scales = [1, 0.5, 0.05]\n",
    "\n",
    "# This tweakable parameter sets the minimum\n",
    "# distance required to accept a step in the MC\n",
    "tolerances = [1, 0.5, 0.2]\n",
    "\n",
    "burned_in_samples_h = []\n",
    "\n",
    "for h, scale in zip(tolerances, scales):\n",
    "    samples, acceptance_rate = sampler_marjoram(x, y, yerr, init_parameters, \n",
    "                                                n_steps, scale=scale, h=h)\n",
    "\n",
    "    burned_in_samples_h.append(samples[n_steps//4:])\n",
    "\n",
    "    print(f\"h = {h:0.2f}, acceptance = {acceptance_rate:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 35\n",
    "\n",
    "histrange = [-0.5, 1.5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.hist(burned_in_samples[:, 1], histtype='step', lw=2, \n",
    "         label='MCMC', density=True, range=histrange, bins=bins)\n",
    "\n",
    "for s, h in zip(burned_in_samples_h, tolerances):\n",
    "    ax.hist(s[:, 0], histtype='step', lw=2, bins=bins,\n",
    "             label=f'LF-MCMC: h={h:.2f}', density=True, range=histrange)\n",
    "ax.legend(loc=(1.01, 0.2))\n",
    "for s in ['right', 'top']:\n",
    "    ax.spines[s].set_visible(False)\n",
    "    \n",
    "ax.set(xlabel='Mean (intercept)', ylabel='PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for large $h$, the posterior has the correct MAP value but the standard deviation/FWHM of the distribution is an overestimate compared to the MCMC posterior. As $h \\rightarrow 0$, the likelihood-free MCMC posterior approaches the same shape as the ordinary MCMC posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
